{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "380d3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paths():\n",
    "    path = Path()\n",
    "\n",
    "    saved_file_generator = path.joinpath(\"data_workaround\").rglob(\"**/part-*\")\n",
    "    paths = [str(path.absolute()) for path in saved_file_generator]\n",
    "    \n",
    "    return paths\n",
    "get_data_paths()  # nothing is hard coded, so don't worry about C:\\\\Lunky...\n",
    "\n",
    "def turn_data_into_df():\n",
    "    \"\"\"\n",
    "    When you run letsgo-win.bat, Seppe have setup variables \"spark\" and \"sc\".\n",
    "    Variable \"spark\" is of the type spark: pyspark.sql.session.SparkSession\n",
    "    \n",
    "    Returns an object of the type spark DataFrame\n",
    "    \"\"\"\n",
    "    path = Path()\n",
    "\n",
    "    saved_file_generator = path.joinpath(\"data_workaround\").rglob(\"**/part-*\")\n",
    "    paths = [str(path.absolute()) for path in saved_file_generator]\n",
    "    \n",
    "    return spark.read.json(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6597e455",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9160\\3410757751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_data_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# data = spark.read.json(\"./data.json\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_data_paths' is not defined"
     ]
    }
   ],
   "source": [
    "data = spark.read.json(get_data_paths())\n",
    "# data = spark.read.json(\"./data.json\")\n",
    "# use the second function if just reading ready data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = turn_data_into_df()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0524809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(app_id='1774580', label=0, review_id='137421224', review_text=\"I'm really loving this game and want to love it even more but the horrible performance on my top end 4080 rig is unacceptable. I'm struggling to get 60fps even without ray tracing and i'd say wait a few weeks to pick this one up. hopefully it'll have a few updates in it by then and everyone can enjoy this great game.\")\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2532bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test, val) = data.randomSplit([0.7, 0.15,0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23051651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first model - tokenizer -> TFHashing -> IDF -> LogReg\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "pipeline1 = Pipeline(stages=[tokenizer, hashtf, idf, lr]).fit(train)\n",
    "predictions1 = pipeline1.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c5ea1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second model - tokenizer -> TFHashing -\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline2 = Pipeline(stages=[tokenizer, cv, idf, lr]).fit(train)\n",
    "predictions2 = pipeline2.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e48e0c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 1: 0.7817\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = predictions1.filter(predictions1.label == predictions1.prediction).count() / float(val.count())\n",
    "print(\"Accuracy Score 1: {0:.4f}\".format(accuracy1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fac6a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 2: 0.7676\n"
     ]
    }
   ],
   "source": [
    "accuracy2 = predictions2.filter(predictions2.label == predictions2.prediction).count() / float(val.count())\n",
    "print(\"Accuracy Score 2: {0:.4f}\".format(accuracy2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83ddf8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline1.write().overwrite().save(\"./lr1.model\")\n",
    "# pipeline2.write().overwrite().save(\"./lr2.model\")\n",
    "pipeline2.save(\"./lr2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "023391d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.46.148.131:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b5058f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of loading \n",
    "saved1 = PipelineModel.load(\"./lr1.model\")\n",
    "saved2 = PipelineModel.load(\"./lr2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bea0bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = saved1.transform(val)\n",
    "predictions4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1bb131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 3: 0.7940\n"
     ]
    }
   ],
   "source": [
    "accuracy3 = predictions3.filter(predictions3.label == predictions3.prediction).count() / float(val.count())\n",
    "print(\"Accuracy Score 3: {0:.4f}\".format(accuracy3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb962c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
